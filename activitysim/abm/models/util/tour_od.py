# ActivitySim
# See full license in LICENSE.txt.
import logging

import pandas as pd

from activitysim.core import tracing
from activitysim.core import config
from activitysim.core import pipeline
from activitysim.core import simulate
from activitysim.core import inject
from activitysim.core import logit
from activitysim.core import orca

from activitysim.core.util import reindex

from activitysim.core.interaction_sample_simulate import interaction_sample_simulate
from activitysim.core.interaction_sample import interaction_sample

from activitysim.core.mem import force_garbage_collect

from . import trip, logsums as logsum

from activitysim.abm.tables.size_terms import tour_destination_size_terms

logger = logging.getLogger(__name__)
DUMP = False


class SizeTermCalculator(object):
    """
    convenience object to provide size_terms for a selector (e.g. non_mandatory)
    for various segments (e.g. tour_type or purpose)
    returns size terms for specified segment in df or series form. Allows this model
    to avoid the use of shadow pricing vis-a-vis shadowPriceCalculator to compute size
    terms
    """

    def __init__(self, size_term_selector):

        # do this once so they can request size_terms for various segments (tour_type or purpose)
        land_use = inject.get_table('land_use')
        size_terms = inject.get_injectable('size_terms')
        self.destination_size_terms = \
            tour_destination_size_terms(land_use, size_terms, size_term_selector)
        self.land_use = land_use

    def omnibus_size_terms_df(self):
        return self.destination_size_terms

    def dest_size_terms_df(self, segment_name):
        # return size terms as df with one column named 'size_term'
        # convenient if creating or merging with alts

        size_terms = self.destination_size_terms[[segment_name]].copy()
        size_terms.columns = ['size_term']
        return size_terms

    def od_size_terms_df(
            self, segment_name, new_index_name=None, origin_id_col='origin',
            dest_id_col='destination', origin_filter=None, origin_attr_cols=None):
        # if sampling/simulating origins and destinations at the same time,
        # size terms df must be the same shape as the full table of alternatives. by
        # default this will be the n x n set of zone-to-zone pairs. optionally, the
        # user can specify a filter to limit the number of origins or destinations
        # such that the resulting data frame will be n x m elements long where m is 
        # the number of zones left (i.e. the length of the land use table) after
        # applying the filter
        size_terms = self.dest_size_terms_df(segment_name)
        land_use = self.land_use.to_frame()
        
        if origin_filter:
            origins = land_use.query(origin_filter)
        else:
            origins = land_use

        n_repeat = len(origins)
        size_terms = size_terms.reindex(size_terms.index.repeat(n_repeat))
        size_terms[origin_id_col] = list(origins.index.values) * len(land_use)
        size_terms.index.name = dest_id_col
        size_terms.reset_index(inplace=True)
            
        if new_index_name:
            size_terms.index.name = new_index_name

        # manually add origin attributes to output since these can't be generated by
        # the destination-based size term calculator
        if origin_attr_cols:
            land_use.index.name = origin_id_col
            land_use.reset_index(inplace=True)
            size_terms.reset_index(inplace=True)
            size_terms = pd.merge(
                size_terms, land_use[origin_attr_cols + [origin_id_col]],
                on=origin_id_col, how='left').set_index(new_index_name)

        return size_terms

    def dest_size_terms_series(self, segment_name):
        # return size terms as as series
        # convenient (and no copy overhead) if reindexing and assigning into alts column
        return self.destination_size_terms[segment_name]


def run_od_sample(
        spec_segment_name,
        tours,
        model_settings,
        network_los,
        od_size_terms,
        estimator,
        chunk_size,
        trace_label):

    model_spec = simulate.spec_for_segment(model_settings, spec_id='SAMPLE_SPEC',
                                           segment_name=spec_segment_name, estimator=estimator)

    choosers = tours
    # FIXME - MEMORY HACK - only include columns actually used in spec
    chooser_columns = model_settings['SIMULATE_CHOOSER_COLUMNS']
    choosers = choosers[chooser_columns]

    alt_od_col_name = model_settings['ALT_OD_COL_NAME']

    logger.info("running %s with %d tours", trace_label, len(choosers))

    sample_size = model_settings['SAMPLE_SIZE']
    if estimator:
        # FIXME interaction_sample will return unsampled complete alternatives with probs and pick_count
        logger.info("Estimation mode for %s using unsampled alternatives short_circuit_choices" % (trace_label,))
        sample_size = 0

    # create wrapper with keys for this lookup - in this case there is a workplace_zone_id
    # in the choosers and a zone_id in the alternatives which get merged during interaction
    # (logit.interaction_dataset suffixes duplicate chooser column with '_chooser')
    # the skims will be available under the name "skims" for any @ expressions
    origin_col_name = model_settings['ORIG_COL_NAME']
    dest_col_name = model_settings['DEST_COL_NAME']

    skim_dict = network_los.get_default_skim_dict()
    skims = skim_dict.wrap(origin_col_name, dest_col_name)

    locals_d = {
        'skims': skims
    }

    constants = config.get_model_constants(model_settings)
    if constants is not None:
        locals_d.update(constants)

    choices = interaction_sample(
        choosers,
        alternatives=od_size_terms,
        sample_size=sample_size,
        alt_col_name=alt_od_col_name,
        spec=model_spec,
        skims=skims,
        locals_d=locals_d,
        chunk_size=chunk_size,
        trace_label=trace_label)

    # remember person_id in chosen alts so we can merge with persons in subsequent steps
    # (broadcasts person_id onto all alternatives sharing the same tour_id index value)
    # choices['person_id'] = choosers.person_id

    return choices


def run_od_logsums(
        spec_segment_name,
        tours_merged_df,
        od_sample,
        model_settings,
        network_los,
        chunk_size,
        trace_hh_id,
        trace_label):
    """
    add logsum column to existing tour_destination_sample table

    logsum is calculated by running the mode_choice model for each sample (person, dest_zone_id) pair
    in destination_sample, and computing the logsum of all the utilities
    """

    logsum_settings = config.read_model_settings(model_settings['LOGSUM_SETTINGS'])

    # FIXME - MEMORY HACK - only include columns actually used in spec
    tours_merged_df = \
        logsum.filter_chooser_columns(tours_merged_df, logsum_settings, model_settings)

    # merge ods into choosers table
    choosers = od_sample.join(tours_merged_df, how='left')

    logger.info("Running %s with %s rows", trace_label, len(choosers))

    tracing.dump_df(DUMP, choosers, trace_label, 'choosers')

    # run trip mode choice to computer tour mode choice logsums
    if logsum_settings.get('COMPUTE_TRIP_MODE_CHOICE_LOGSUMS', False):

        trip_mode_choice_settings = config.read_model_settings('trip_mode_choice')

        # tours_merged table doesn't yet have all the cols it needs to be called (e.g. 
        # home_zone_id), so in order to compute tour mode choice/trip mode choice logsums
        # in this step we have to pass all tour-level attributes in with the main trips
        # table. see trip_mode_choice.py L56-61 for more details.
        tour_cols_needed = trip_mode_choice_settings.get('TOURS_MERGED_CHOOSER_COLUMNS', [])

        # from tour_mode_choice.py
        not_university = (choosers.tour_type != 'school') | ~choosers.is_university
        choosers['tour_purpose'] = \
            choosers.tour_type.where(not_university, 'univ')

        choosers['stop_frequency'] = '0out_0in'
        choosers['primary_purpose'] = choosers['tour_purpose']
        choosers_og_index = choosers.index.name
        choosers.reset_index(inplace=True)
        choosers.index.name = 'unique_id'
        trips = trip.initialize_from_tours(choosers, ['tour_od_id', 'unique_id'])
        outbound = trips['outbound']
        trips['depart'] = reindex(choosers.start, trips.unique_id)
        trips.loc[~outbound, 'depart'] = reindex(choosers.end, trips.loc[~outbound, 'unique_id'])
        

        logsum_trips = pd.DataFrame()
        nest_spec = config.get_logit_model_settings(logsum_settings)

        # actual coeffs dont matter here, just need them to load the nest structure
        coefficients = simulate.get_segment_coefficients(
            logsum_settings, choosers.iloc[0]['tour_purpose'])
        nest_spec = simulate.eval_nest_coefficients(nest_spec, coefficients, trace_label)
        tour_mode_alts = []
        for nest in logit.each_nest(nest_spec):
            if nest.is_leaf:
                tour_mode_alts.append(nest.name)

        # repeat rows from the trips table iterating over tour mode
        for tour_mode in tour_mode_alts:
            trips['tour_mode'] = tour_mode
            logsum_trips = pd.concat((logsum_trips, trips), ignore_index=True)
        assert len(logsum_trips) == len(trips) * len(tour_mode_alts)
        logsum_trips.index.name = 'trip_id'

        for col in tour_cols_needed:
            if col not in trips:
                logsum_trips[col] = reindex(choosers[col], logsum_trips.unique_id)

        pipeline.replace_table('trips', logsum_trips)
        tracing.register_traceable_table('trips', logsum_trips)
        pipeline.get_rn_generator().add_channel('trips', logsum_trips)

        choosers.set_index(choosers_og_index, drop=True, inplace=True)

        # run trip mode choice on pseudo-trips. use orca instead of pipeline to
        # execute the step because pipeline can only handle one open step at a time
        orca.run(['trip_mode_choice'])

        # grab trip mode choice logsums and pivot by tour mode and direction, index
        # on tour_id to enable merge back to choosers table
        trips = inject.get_table('trips').to_frame()
        trip_dir_mode_logsums = trips.pivot(
            index=['tour_id', 'tour_od_id'], columns=['tour_mode', 'outbound'], values='trip_mode_choice_logsum')
        new_cols = [
            '_'.join(['logsum', mode, 'outbound' if outbound else 'inbound'])
            for mode, outbound in trip_dir_mode_logsums.columns]
        trip_dir_mode_logsums.columns = new_cols
        choosers.reset_index(inplace=True)
        choosers.set_index(['tour_id', 'tour_od_id'], inplace=True)
        choosers = pd.merge(choosers, trip_dir_mode_logsums, left_index=True, right_index=True)
        choosers.reset_index(inplace=True)
        choosers.set_index(choosers_og_index, inplace=True)
        pipeline.get_rn_generator().drop_channel('trips')

    logsums = logsum.compute_logsums(
        choosers,
        spec_segment_name,
        logsum_settings,
        model_settings,
        network_los,
        chunk_size,
        trace_label, 'start', 'end', 'duration')

    od_sample['tour_mode_choice_logsum'] = logsums

    return od_sample


def run_od_simulate(
        spec_segment_name,
        tours,
        od_sample,
        model_settings,
        network_los,
        od_size_terms,
        estimator,
        chunk_size,
        trace_label):
    """
    run destination_simulate on tour_destination_sample
    annotated with mode_choice logsum to select a destination from sample alternatives
    """

    model_spec = simulate.spec_for_segment(model_settings, spec_id='SPEC',
                                           segment_name=spec_segment_name, estimator=estimator)

    # merge persons into tours
    choosers = tours

    # FIXME - MEMORY HACK - only include columns actually used in spec
    chooser_columns = model_settings['SIMULATE_CHOOSER_COLUMNS']
    choosers = choosers[chooser_columns]
    if estimator:
        estimator.write_choosers(choosers)

    alt_od_col_name = model_settings['ALT_OD_COL_NAME']
    origin_col_name = model_settings['ORIG_COL_NAME']
    dest_col_name = model_settings['DEST_COL_NAME']

    # alternatives are pre-sampled and annotated with logsums and pick_count
    # but we have to merge size_terms column into alt sample list
    if od_size_terms.index.name in od_size_terms.columns:
        del od_size_terms[od_size_terms.index.name]
    od_sample = od_sample.merge(
        od_size_terms, how='left', left_on='tour_od_id', right_index=True, suffixes=('','_st'))
    if origin_col_name + '_st' in od_sample.columns:
        if all(od_sample[origin_col_name + '_st'] == od_sample[origin_col_name]):
            del od_sample[origin_col_name + '_st']
        else:
            logger.error('origin column values od_sample df need reconciling')
        if all(od_sample[dest_col_name + '_st'] == od_sample[dest_col_name]):
            del od_sample[dest_col_name + '_st']
        else:
            logger.error('destination column values in od_sample df need reconciling')

    tracing.dump_df(DUMP, od_sample, trace_label, 'alternatives')

    constants = config.get_model_constants(model_settings)

    logger.info("Running tour_destination_simulate with %d persons", len(choosers))

    # create wrapper with keys for this lookup - in this case there is a home_zone_id in the choosers
    # and a zone_id in the alternatives which get merged during interaction
    # the skims will be available under the name "skims" for any @ expressions
    skim_dict = network_los.get_default_skim_dict()
    skims = skim_dict.wrap(origin_col_name, dest_col_name)

    locals_d = {
        'skims': skims,
    }
    if constants is not None:
        locals_d.update(constants)

    tracing.dump_df(DUMP, choosers, trace_label, 'choosers')

    choices = interaction_sample_simulate(
        choosers,
        od_sample,
        spec=model_spec,
        choice_column=alt_od_col_name,
        skims=skims,
        locals_d=locals_d,
        chunk_size=chunk_size,
        trace_label=trace_label,
        trace_choice_name='destination',
        estimator=estimator)

    choices = choices.to_frame('choice')

    return choices


def get_od_cols_from_od_id(choices_df, reference_df, orig_col_name, dest_col_name, choice_col='choice'):
    
    choices_df[orig_col_name] = reference_df.reindex(choices_df[choice_col])[orig_col_name].values
    choices_df[dest_col_name] = reference_df.reindex(choices_df[choice_col])[dest_col_name].values

    return choices_df


def run_tour_od(
        tours,
        persons,
        want_sample_table,
        model_settings,
        network_los,
        estimator,
        chunk_size, trace_hh_id, trace_label):

    size_term_calculator = SizeTermCalculator(model_settings['SIZE_TERM_SELECTOR'])
    size_term_index_name = model_settings['ALT_OD_COL_NAME']
    origin_col_name = model_settings['ORIG_COL_NAME']
    dest_col_name = model_settings['DEST_COL_NAME']
    
    origin_filter = None
    if model_settings['ORIG_FILTER']:
        origin_filter = model_settings['ORIG_FILTER']

    chooser_segment_column = model_settings['CHOOSER_SEGMENT_COLUMN_NAME']

    # maps segment names to compact (integer) ids
    segments = model_settings['SEGMENTS']

    # interaction_sample_simulate insists choosers appear in same order as alts
    tours = tours.sort_index()

    choices_list = []
    sample_list = []
    for segment_name in segments:

        choosers = tours[tours[chooser_segment_column] == segment_name]

        choosers = pd.merge(
            choosers, persons.to_frame(columns=['is_university', 'demographic_segment']),
            left_on='person_id', right_index=True)

        # size_term segment is segment_name
        origin_attr_cols = model_settings['ORIGIN_ATTR_COLS_TO_USE']
        segment_od_size_terms = size_term_calculator.od_size_terms_df(
            segment_name, size_term_index_name, origin_col_name, dest_col_name,
            origin_filter, origin_attr_cols)

        if choosers.shape[0] == 0:
            logger.info("%s skipping segment %s: no choosers", trace_label, segment_name)
            continue

        # - od_sample
        spec_segment_name = segment_name  # spec_segment_name is segment_name
        od_sample_df = \
            run_od_sample(
                spec_segment_name,
                choosers,
                model_settings,
                network_los,
                segment_od_size_terms,
                estimator,
                chunk_size=chunk_size,
                trace_label=tracing.extend_trace_label(trace_label, 'sample.%s' % segment_name))
        od_sample_df = get_od_cols_from_od_id(
            od_sample_df, segment_od_size_terms, origin_col_name, dest_col_name, 'tour_od_id')

        # - destination_logsums
        od_sample_df = \
            run_od_logsums(
                spec_segment_name,
                choosers,
                od_sample_df,
                model_settings,
                network_los,
                chunk_size=chunk_size,
                trace_hh_id=trace_hh_id,
                trace_label=tracing.extend_trace_label(trace_label, 'logsums.%s' % segment_name))

        # - od_simulate
        choices = \
            run_od_simulate(
                spec_segment_name,
                choosers,
                od_sample=od_sample_df,
                model_settings=model_settings,
                network_los=network_los,
                od_size_terms=segment_od_size_terms,
                estimator=estimator,
                chunk_size=chunk_size,
                trace_label=tracing.extend_trace_label(trace_label, 'simulate.%s' % segment_name))

        choices[origin_col_name] = segment_od_size_terms.reindex(choices.choice)[origin_col_name].values
        choices[dest_col_name] = segment_od_size_terms.reindex(choices.choice)[dest_col_name].values

        choices_list.append(choices)

        if want_sample_table:
            # FIXME - sample_table
            od_sample_df.set_index(model_settings['ALT_DEST_COL_NAME'],
                                         append=True, inplace=True)
            sample_list.append(location_sample_df)

        # FIXME - want to do this here?
        del od_sample_df
        force_garbage_collect()

    if len(choices_list) > 0:
        choices_df = pd.concat(choices_list)

    if len(sample_list) > 0:
        save_sample_df = pd.concat(sample_list)
    else:
        # this could happen either with small samples as above, or if no saved sample desired
        save_sample_df = None

    return choices_df, save_sample_df
