
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Software Development &#8212; ActivitySim 1.1.1.dev41+g4944aef4</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/autodoc_pydantic.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    <link rel="stylesheet" type="text/css" href="_static/theme_overrides.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/design-tabs.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Models" href="models.html" />
    <link rel="prev" title="Developer Installation" href="dev-guide/install.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="index.html">
<p class="title">ActivitySim</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="users-guide/index.html">
  Users Guide
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="dev-guide/index.html">
  Developers
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <div class="dropdown" id="version_switcher">
    <button type="button" class="btn btn-primary btn-sm navbar-btn dropdown-toggle" id="version_switcher_button" data-toggle="dropdown">
        1.1.1  <!-- this text may get changed later by javascript -->
        <span class="caret"></span>
    </button>
    <div id="version_switcher_menu" class="dropdown-menu list-group-flush py-0" aria-labelledby="version_switcher_button">
    <!-- dropdown will be populated by javascript on page load -->
    </div>
</div>

<!-- NOTE: this JS must live here (not in our global JS file) because it relies
     on being processed by Jinja before it is run (specifically for replacing
     variables development and {'json_url': 'https://activitysim.github.io/activitysim/switcher.json', 'version_match': '1.1.1'}.
-->

<script type="text/javascript">
// Check if corresponding page path exists in other version of docs
// and, if so, go there instead of the homepage of the other docs version
function checkPageExistsAndRedirect(event) {
    const currentFilePath = "development.html",
          tryUrl = event.target.getAttribute("href");
    let otherDocsHomepage = tryUrl.replace(currentFilePath, "");
    $.ajax({
        type: 'HEAD',
        url: tryUrl,
        // if the page exists, go there
        success: function() {
            location.href = tryUrl;
        }
    }).fail(function() {
        location.href = otherDocsHomepage;
    });
    // this prevents the browser from following the href of the clicked node
    // (which is fine because this function takes care of redirecting)
    return false;
}

// Populate the version switcher from the JSON config file
(function () {
    $.getJSON("https://activitysim.github.io/activitysim/switcher.json", function(data, textStatus, jqXHR) {
        const currentFilePath = "development.html";
        // create links to the corresponding page in the other docs versions
        $.each(data, function(index, entry) {
            // if no custom name specified (e.g., "latest"), use version string
            if (!("name" in entry)) {
                entry.name = entry.version;
            }
            // create the node
            const node = document.createElement("a");
            node.setAttribute("class", "list-group-item list-group-item-action py-1");
            node.textContent = `${entry.name}`;
            node.setAttribute("href", `${entry.url}${currentFilePath}`);
            // on click, AJAX calls will check if the linked page exists before
            // trying to redirect, and if not, will redirect to the homepage
            // for that version of the docs.
            node.onclick = checkPageExistsAndRedirect;
            // Add dataset values for the version and name in case people want
            // to apply CSS styling based on this information.
            node.dataset["versionName"] = entry.name;
            node.dataset["version"] = entry.version;

            $("#version_switcher_menu").append(node);
            // replace dropdown button text with the preferred display name of
            // this version, rather than using sphinx's 1.1.1 variable.
            // also highlight the dropdown entry for the currently-viewed
            // version's entry
            if (entry.version == "1.1.1") {
                node.classList.add("active");
                let btn = document.getElementById("version_switcher_button");
                btn.innerText = btn.dataset["activeVersionName"] = entry.name;
                btn.dataset["activeVersion"] = entry.version;
            }
        });
    });
})();
</script>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar">
              <div class="sidebar-start-items"><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="dev-guide/install.html">
   Developer Installation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Software Development
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="models.html">
   Models
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="core.html">
   Core Components
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="benchmarking.html">
   Benchmarking
  </a>
 </li>
</ul>

  </div>
</nav>
              </div>
              <div class="sidebar-end-items">
              </div>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage mt-5 pt-1 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#software-design">
   Software Design
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#model-orchestrator">
     Model Orchestrator
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#data-handling">
     Data Handling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#expressions">
     Expressions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#choice-models">
     Choice Models
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#person-time-windows">
     Person Time Windows
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#models">
     Models
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#development-install">
   Development Install
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#development-guidelines">
   Development Guidelines
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#style">
     Style
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#imports">
     Imports
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#working-together-in-the-repository">
     Working Together in the Repository
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#versioning">
     Versioning
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#testing">
     Testing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#profiling">
     Profiling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#documentation">
     Documentation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#releases">
     Releases
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#issues-and-support">
     Issues and Support
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#license">
     License
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#contribution-review-criteria">
     Contribution Review Criteria
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#adding-agency-examples">
   Adding Agency Examples
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples">
     Examples
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     Testing
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#running-the-test-system">
     Running the Test System
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#update-use-cases">
     Update Use Cases
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="software-development">
<h1>Software Development<a class="headerlink" href="#software-development" title="Permalink to this headline">#</a></h1>
<p>This page documents the ActivitySim software design and how to contribute to the project.</p>
<section id="software-design">
<h2>Software Design<a class="headerlink" href="#software-design" title="Permalink to this headline">#</a></h2>
<p>The core software components of ActivitySim are described below.  ActivitySim is
implemented in Python, and makes heavy use of the vectorized backend C/C++ libraries in
<a class="reference external" href="http://pandas.pydata.org">pandas</a>  and <a class="reference external" href="http://numpy.org">numpy</a> in order to be quite performant.
The core design principle of the system is vectorization of for loops, and this principle
is woven into the system wherever reasonable.  As a result, the Python portions of the software
can be thought of as more of an orchestrator, data processor, etc. that integrates a series of
C/C++ vectorized data table and matrix operations.  The model system formulates
each simulation as a series of vectorized table operations and the Python layer
is responsible for setting up and providing expressions to operate on these large data tables.</p>
<p>In developing this software platform, we strive to adhere to a best practices approach to scientific computing,
as summarized in <a class="reference external" href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001745">this article.</a></p>
<section id="model-orchestrator">
<h3>Model Orchestrator<a class="headerlink" href="#model-orchestrator" title="Permalink to this headline">#</a></h3>
<p>An ActivitySim model is a sequence of model / data processing steps, commonly known as a data pipeline.
A well defined data pipeline has the ability to resume jobs at a known point, which facilitates
debugging of problems with data and/or calculations.  It also allows for checkpointing model
resources, such as the state of each person at a point in the model simulation.  Checkpointing also
allows for regression testing of results at specified points in overall model run.</p>
<p>ActivitySim’s model orchestrator makes use of depedency injection, which is where one object (or method)
supplies the dependencies of another object.  Dependency injection is done by the <a class="reference internal" href="core.html#module-activitysim.core.inject" title="activitysim.core.inject"><code class="xref py py-mod docutils literal notranslate"><span class="pre">activitysim.core.inject</span></code></a>
module, which wraps <a class="reference external" href="https://github.com/udst/orca">ORCA</a>, an orchestration/pipeline tool.  Inject defines model
steps, dynamic data sources, and connects them to processing functions. It also defines dynamic data tables
based on pandas DataFrames, columns based on pandas Series, and injectables (functions).  Model steps are executed
as steps registered with the model orchestration engine.  Over time Inject has extended ORCA’s functionality by
adding a <a class="reference internal" href="core.html#pipeline-in-detail"><span class="std std-ref">Pipeline</span></a> that runs a series of model steps, manages the state of the data
tables throughout the model run, allows for restarting at any model step, and integrates with the
random number generation procedures (see <a class="reference internal" href="core.html#random-in-detail"><span class="std std-ref">Random</span></a>).</p>
</section>
<section id="data-handling">
<h3>Data Handling<a class="headerlink" href="#data-handling" title="Permalink to this headline">#</a></h3>
<p>ActivitySim works with three open data formats, <a class="reference external" href="https://www.hdfgroup.org/HDF5/">HDF5</a>
, <a class="reference external" href="https://github.com/osPlanning/omx">Open Matrix (OMX)</a>, and <a class="reference external" href="https://en.wikipedia.org/wiki/Comma-separated_values">CSV</a> .
The HDF5 binary data container is used for the <a class="reference internal" href="core.html#pipeline-in-detail"><span class="std std-ref">Pipeline</span></a> data store.
OMX, which is based on HDF5, is used for input and output matrices (skims and demand matrices).  CSV files
are used for various inputs and outputs as well.</p>
<p>Three key data structures in ActivitySim are:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html">pandas.DataFrame</a> - A data table with rows and columns, similar to an R data frame, Excel worksheet, or database table</p></li>
<li><p><a class="reference external" href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html">pandas.Series</a> - a vector of data, a column in a DataFrame table or a 1D array</p></li>
<li><p><a class="reference external" href="http://docs.scipy.org/doc/numpy/reference/arrays.html">numpy.array</a> - an N-dimensional array of items of the same type, and is often a network skim matrix or collection of skim matrices by time-period or mode for example</p></li>
</ul>
</section>
<section id="expressions">
<h3>Expressions<a class="headerlink" href="#expressions" title="Permalink to this headline">#</a></h3>
<p>ActivitySim exposes all model expressions in CSV files.  These model expression CSV files
contain Python expressions, mainly pandas/numpy expression, and reference input data tables
and network skim matrices.  With this design, the Python code, which can be thought of as a generic expression
engine, and the specific model calculations, such as the utilities, are separate.  This helps to avoid
modifying the actual Python code when making changes to the models, such as during model calibration. An
example of model expressions is found in the example auto ownership model specification file -
<a class="reference external" href="https://github.com/activitysim/activitysim/blob/main/example/configs/auto_ownership.csv">auto_ownership.csv</a>.
Refer to the <a class="reference internal" href="core.html#expressions"><span class="std std-ref">Utility Expressions</span></a> section for more detail.</p>
<p>Many of the models have pre- and post-processor table annotators, which read a CSV file of expression, calculate
required additional table fields, and join the fields to the target tables.  An example table annotation expressions
file is found in the example configuration files for households for the CDAP model -
<a class="reference external" href="https://github.com/activitysim/activitysim/blob/main/example/configs/annotate_households_cdap.csv">annotate_households_cdap.csv</a>.
Refer to <a class="reference internal" href="models.html#table-annotation"><span class="std std-ref">Estimation</span></a> for more information and the <code class="xref py py-func docutils literal notranslate"><span class="pre">activitysim.abm.models.util.expressions.assign_columns()</span></code> function.</p>
</section>
<section id="choice-models">
<h3>Choice Models<a class="headerlink" href="#choice-models" title="Permalink to this headline">#</a></h3>
<p>ActivitySim currently supports multinomial (MNL) and nested logit (NL) choice models. Refer to <a class="reference internal" href="core.html#logit-in-detail"><span class="std std-ref">Logit</span></a>
for more information.  It also supports custom expressions as noted above, which can often be used to
code additional types of choice models.  In addition, developers can write their own choice models
in Python and expose these through the framework.</p>
</section>
<section id="person-time-windows">
<h3>Person Time Windows<a class="headerlink" href="#person-time-windows" title="Permalink to this headline">#</a></h3>
<p>The departure time and duration models require person time windows. Time windows are adjacent time
periods that are available for travel. ActivitySim maintains time windows in a pandas table where each row is
a person and each time period is a column.  As travel is scheduled throughout the simulation, the relevant
columns for the tour, trip, etc. are updated as needed. Refer to <a class="reference internal" href="core.html#time-windows"><span class="std std-ref">Person Time Windows</span></a> for more information.</p>
</section>
<section id="models">
<h3>Models<a class="headerlink" href="#models" title="Permalink to this headline">#</a></h3>
<p>An activitysim travel model is made up of a series of models, or steps in the data pipeline.  A model
typically does the following:</p>
<blockquote>
<div><ul class="simple">
<li><p>registers an Inject step that is called by the model runner</p></li>
<li><p>sets up logging and tracing</p></li>
<li><p>gets the relevant input data tables from Inject</p></li>
<li><p>gets all required settings, config files, etc.</p></li>
<li><p>runs a data preprocessor on each input table that needs additional fields for the calculation</p></li>
<li><p>solves the model in chunks of data table rows if needed</p></li>
<li><p>runs a data postprocessor on the output table data that needs additional fields for later models</p></li>
<li><p>writes the resulting table data to the pipeline</p></li>
</ul>
</div></blockquote>
<p>See <a class="reference internal" href="models.html#models"><span class="std std-ref">Models</span></a> for more information.</p>
</section>
</section>
<section id="development-install">
<h2>Development Install<a class="headerlink" href="#development-install" title="Permalink to this headline">#</a></h2>
<p>The development version of ActivitySim can be installed as follows:</p>
<ul class="simple">
<li><p>Clone or fork the source from the <a class="reference external" href="https://github.com/activitysim/activitysim">GitHub repository</a></p></li>
<li><p>Navigate to your local activitysim git directory</p></li>
<li><p>Create a development environment by running
<code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">env</span> <span class="pre">create</span> <span class="pre">--file=conda-environments/activitysim-dev.yml</span> <span class="pre">--name</span> <span class="pre">ASIM_DEV</span></code>.
This will create a new conda environment named “ASIM_DEV” (change the name in
the command if desired). ActivitySim will be installed in “editable” mode, so
any changes you make in the code in your git directory will be reflected.</p></li>
<li><p>Activate the new conda environment <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">ASIM_DEV</span></code></p></li>
</ul>
</section>
<section id="development-guidelines">
<h2>Development Guidelines<a class="headerlink" href="#development-guidelines" title="Permalink to this headline">#</a></h2>
<p>ActivitySim development adheres to the following standards.</p>
<section id="style">
<h3>Style<a class="headerlink" href="#style" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Python code should follow the <a class="reference external" href="https://pypi.python.org/pypi/pycodestyle">pycodestyle style guide</a></p></li>
<li><p>Python docstrings should follow the <a class="reference external" href="https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt">numpydoc documentation format</a></p></li>
</ul>
</section>
<section id="imports">
<h3>Imports<a class="headerlink" href="#imports" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Imports should be one per line.</p></li>
<li><p>Imports should be grouped into standard library, third-party, and intra-library imports.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">from</span></code> import should follow regular <code class="docutils literal notranslate"><span class="pre">imports</span></code>.</p></li>
<li><p>Within each group the imports should be alphabetized.</p></li>
<li><p>Imports of scientific Python libraries should follow these conventions:</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>
</div>
</section>
<section id="working-together-in-the-repository">
<h3>Working Together in the Repository<a class="headerlink" href="#working-together-in-the-repository" title="Permalink to this headline">#</a></h3>
<p>We use <a class="reference external" href="https://guides.github.com/introduction/flow">GitHub Flow</a>.  The key points to
our GitHub workflow are:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">main</span></code> branch contains the latest release version of the ActivitySim resources</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">develop</span></code> branch contains new features or revisions planned for the next release.
Generally, developers should not work directly in the <code class="docutils literal notranslate"><span class="pre">develop</span></code> branch.</p></li>
<li><p>Work to implement new features or other revisions is done in an issue/feature branch
(or a fork) and developers can open a pull request (PR) to merge their work into <code class="docutils literal notranslate"><span class="pre">develop</span></code>.</p></li>
<li><p>The test system automatically runs the tests on PR’s.  PR’s do not necessarily need to pass
all tests to be merged into <code class="docutils literal notranslate"><span class="pre">develop</span></code>, but any failures should be cause by known existing
problems – PR’s should strive to not break anything beyond what was broken previously.</p></li>
<li><p>Upon review and agreement by a consortium member or committer other than the author,
and barring any objection raised by a consortium member, PR’s can be merged into the
<code class="docutils literal notranslate"><span class="pre">develop</span></code> branch.</p></li>
<li><p>If tests pass for the <code class="docutils literal notranslate"><span class="pre">develop</span></code> branch, new features are suitably documented, and on approval of
<a class="reference external" href="https://github.com/ActivitySim/activitysim/wiki/Governance#actions">a lazy majority of the PMC</a>,
a repository administrator can approve a manual pull request to merge <code class="docutils literal notranslate"><span class="pre">develop</span></code> into <code class="docutils literal notranslate"><span class="pre">main</span></code>,
and otherwise make a <a class="reference external" href="https://github.com/ActivitySim/activitysim/blob/main/HOW_TO_RELEASE.md">product release</a>.</p></li>
</ul>
</section>
<section id="versioning">
<h3>Versioning<a class="headerlink" href="#versioning" title="Permalink to this headline">#</a></h3>
<p>ActivitySim uses the following <a class="reference external" href="http://the-hitchhikers-guide-to-packaging.readthedocs.io/en/latest/specification.html">versioning convention</a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">MAJOR</span><span class="o">.</span><span class="n">MINOR</span><span class="o">.</span><span class="n">PATCH</span><span class="p">[</span><span class="o">.</span><span class="n">devN</span><span class="p">]</span>
</pre></div>
</div>
<ul class="simple">
<li><p>where MAJOR designates a major revision number for the software, like 2 or 3 for Python.
Usually, raising a major revision number means that you are adding a lot of features,
breaking backward-compatibility or drastically changing the APIs (Application Program
Interface) or ABIs (Application Binary Interface).</p></li>
<li><p>MINOR usually groups moderate changes to the software like bug fixes or minor improvements.
Most of the time, end users can upgrade with no risks their software to a new minor release.
In case an API changes, the end users will be notified with deprecation warnings. In other
words, API and ABI stability is usually a promise between two minor releases.</p></li>
<li><p>PATCH releases are made principally to address bugs or update non-core parts of the
ActivitySim codebase (e.g. dependency requirements, distribution channels). End users
should expect no changes at all in how the software works between two patch releases.</p></li>
<li><p>DEVELOPMENT pre-releases are used to test and prepare integration with other external
services that require a “release”. End users should not typically install or use a development
release other than for a specific well-defined purpose.</p></li>
</ul>
</section>
<section id="testing">
<h3>Testing<a class="headerlink" href="#testing" title="Permalink to this headline">#</a></h3>
<p>ActivitySim testing is done with three tools:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://pypi.python.org/pypi/pycodestyle">pycodestyle</a>, a tool to check Python code against the pycodestyle style conventions</p></li>
<li><p><a class="reference external" href="http://pytest.org/latest/">pytest</a>, a Python testing tool</p></li>
<li><p><a class="reference external" href="https://github.com/coagulant/coveralls-python">coveralls</a>, a tool for measuring code coverage and publishing code coverage stats online</p></li>
</ul>
<p>To run the tests locally, first make sure the required packages are installed.  Next, run the tests with the following commands:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pycodestyle</span>
<span class="n">py</span><span class="o">.</span><span class="n">test</span>
</pre></div>
</div>
<p>These same tests are run by Travis with each push to the repository.  These tests need to pass in order
to merge the revisions into main.</p>
<p>In some cases, test targets need to be updated to match the new results produced by the code since these
are now the correct results.  In order to update the test targets, first determine which tests are
failing and then review the failing lines in the source files.  These are easy to identify since each
test ultimately comes down to one of Python’s various types of <cite>assert</cite> statements.  Once you identify
which <cite>assert</cite> is failing, you can work your way back through the code that creates the test targets in
order to update it.  After updating the test targets, re-run the tests to confirm the new code passes all
the tests.</p>
<p>See <a class="reference internal" href="#adding-agency-examples"><span class="std std-ref">Adding Agency Examples</span></a> for more information on testing, most notably, agency example models.</p>
</section>
<section id="profiling">
<h3>Profiling<a class="headerlink" href="#profiling" title="Permalink to this headline">#</a></h3>
<p>A handy way to profile ActivitySim model runs is to use <a class="reference external" href="https://jiffyclub.github.io/snakeviz/">snakeviz</a>.
To do so, first install snakeviz and then run ActivitySim with the Python profiler (cProfile) to create
a profiler file.  Then run snakeviz on the profiler file to interactively explore the component runtimes.</p>
</section>
<section id="documentation">
<h3>Documentation<a class="headerlink" href="#documentation" title="Permalink to this headline">#</a></h3>
<p>The documentation is written in <a class="reference external" href="http://docutils.sourceforge.net/rst.html">reStructuredText</a> markup
and built with <a class="reference external" href="http://www.sphinx-doc.org/en/stable/">Sphinx</a>.  In addition to converting rst files
to html and other document formats, these tools also read the inline Python docstrings and convert
them into html as well.  ActivitySim’s docstrings are written in <a class="reference external" href="https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt">numpydoc format</a> since it is easier to use
than standard rst format.</p>
<p>To build the documentation, first make sure the required packages are installed.  Next, build the
documentation in html format with the <code class="docutils literal notranslate"><span class="pre">make</span> <span class="pre">html</span></code> command run from the <code class="docutils literal notranslate"><span class="pre">docs</span></code> folder.</p>
<p>If the activitysim package is installed, then the documentation will be built from that version of
the source code instead of the git repo version.  When pushing revisions to the repo, the documentation
is automatically built by Travis after successfully passing the tests.</p>
<p>GitHub automatically publishes the gh-pages branch at <a class="reference external" href="https://activitysim.github.io/activitysim">https://activitysim.github.io/activitysim</a>.</p>
</section>
<section id="releases">
<span id="release-steps"></span><h3>Releases<a class="headerlink" href="#releases" title="Permalink to this headline">#</a></h3>
<p>With the agreement of the PMC, a project administrator will handle making releases, following the detailed
steps outlined in the <a class="reference external" href="https://github.com/ActivitySim/activitysim/blob/main/HOW_TO_RELEASE.md">HOW_TO_RELEASE</a>
document.</p>
</section>
<section id="issues-and-support">
<h3>Issues and Support<a class="headerlink" href="#issues-and-support" title="Permalink to this headline">#</a></h3>
<p>Issue tracking and support is done through GitHub <a class="reference external" href="https://github.com/ActivitySim/activitysim/issues">issues</a>.</p>
</section>
<section id="license">
<h3>License<a class="headerlink" href="#license" title="Permalink to this headline">#</a></h3>
<p>ActivitySim is provided “as is.”  See the
<a class="reference external" href="https://github.com/ActivitySim/activitysim/blob/main/LICENSE.txt">License</a> for more information.</p>
</section>
<section id="contribution-review-criteria">
<h3>Contribution Review Criteria<a class="headerlink" href="#contribution-review-criteria" title="Permalink to this headline">#</a></h3>
<p>When contributing to ActivitySim, the set of questions below will be asked of the contribution.  Make sure to also
review the documentation above before making a submittal.  The automated test system also provides some helpful
information where identified.</p>
<p>To submit a contribution for review, issue a pull request with a comment introducing your contribution.  The comment
should include a brief overview, responses to the questions, and pointers to related information.  The entire submittal
should ideally be self contained so any additional documentation should be in the pull request as well.
The <a class="reference external" href="https://github.com/ActivitySim/activitysim/wiki/Governance#project-management-committee-pmc">PMC</a> and/or its Contractor will handle the review request, comment on each
question, complete the feedback form, and reply to the pull request.  If accepted, the commit(s) will
be <a class="reference external" href="https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-request-merges#squash-and-merge-your-pull-request-commits">squashed and merged</a>.
Its a good idea to setup a pre-submittal meeting to discuss questions and better understand expectations.</p>
<p><strong>Review Criteria</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Does it contain all the required elements, including a runnable example, documentation, and tests?</p></li>
<li><p>Does it implement good methods (i.e. is it consistent with good practices in travel modeling)?</p></li>
<li><p>Are the runtimes reasonable and does it provide documentation justifying this claim?</p></li>
<li><p>Does it include non-Python code, such as C/C++?  If so, does it compile on any OS and are compilation instructions included?</p></li>
<li><p>Is it licensed with the ActivitySim license that allows the code to be freely distributed and modified and includes attribution so that the provenance of the code can be tracked? Does it include an official release of ownership from the funding agency if applicable?</p></li>
<li><p>Does it appropriately interact with the data pipeline (i.e. it doesn’t create new ways of managing data)?</p></li>
<li><p>Does it include regression tests to enable checking that consistent results will be returned when updates are made to the framework?</p></li>
<li><p>Does it include sufficient test coverage and test data for existing and proposed features?</p></li>
<li><p>Any other comments or suggestions for improving the developer experience?</p></li>
</ol>
</div></blockquote>
<p><strong>Feedback</strong></p>
<p>The PMC and/or its Contractor will provide feedback for each review criteria above and tag each submittal category as follows:</p>
<table class="table">
<colgroup>
<col style="width: 41%" />
<col style="width: 15%" />
<col style="width: 22%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Status</p></th>
<th class="head"><p>Code</p></th>
<th class="head"><p>Documentation</p></th>
<th class="head"><p>Tests/Examples</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Accept</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>Accept but recommend revisions</p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>Do not accept</p></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="adding-agency-examples">
<span id="id1"></span><h2>Adding Agency Examples<a class="headerlink" href="#adding-agency-examples" title="Permalink to this headline">#</a></h2>
<p>ActivitySim includes several mature or in-development full scale agency <a class="reference internal" href="examples.html#examples"><span class="std std-ref">Examples</span></a>.  Adding an agency example to
ActivitySim adds additional assurances that future updates to ActivitySim will more easily work for users.  At the same
time, with each additional implementation, the need for additional test coverage increases.  This increased need for
test coverage relates to when setting up a new model, with differences in inputs and configurations, when adding new
model components (and/or revisions to the core) in order to implement new features, and when implementing model
components at a scale previously untested.  The following section describes the process to add an agency example model
to ActivitySim.</p>
<section id="examples">
<h3>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">#</a></h3>
<p>Generally speaking, there are two types of ActivitySim examples: test examples and agency examples.</p>
<ul class="simple">
<li><p>Test examples - these are the core ActivitySim maintained and tested examples developed to date.  The current test
examples are <a class="reference internal" href="examples.html#prototype-mtc"><span class="std std-ref">prototype_mtc</span></a>, <a class="reference internal" href="examples.html#example-estimation"><span class="std std-ref">example_estimation</span></a>, <a class="reference internal" href="examples.html#placeholder-multiple-zone"><span class="std std-ref">placeholder_multiple_zone</span></a>, and <a class="reference internal" href="examples.html#prototype-marin"><span class="std std-ref">prototype_marin</span></a>.
These examples are owned and maintained by the project.</p></li>
<li><p>Agency examples - these are agency partner model implementations currently being setup.  The current agency examples
are <a class="reference internal" href="examples.html#prototype-arc"><span class="std std-ref">prototype_arc</span></a>, <a class="reference internal" href="examples.html#prototype-semcog"><span class="std std-ref">prototype_semcog</span></a>, <a class="reference internal" href="examples.html#placeholder-psrc"><span class="std std-ref">placeholder_psrc</span></a>, <a class="reference internal" href="examples.html#placeholder-sandag"><span class="std std-ref">placeholder_sandag</span></a>, and <a class="reference internal" href="examples.html#prototype-sandag-xborder"><span class="std std-ref">prototype_sandag_xborder</span></a>.  These examples can be
configured in ways different from the test examples, include new inputs and expressions, and may include new planned
software components for contribution to ActivitySim.  These examples are owned by the agency.</p></li>
</ul>
<p>Furthermore, multiple versions of these examples can exist, and be used for various testing purposes:</p>
<ul class="simple">
<li><p>Full scale - a full scale data setup, including all households, zones, skims, time periods, etc.  This is a “typical”
model setup used for application.  This setup can be used to test the model results and performance since model
results can be compared to observed/known answers and runtimes can be compared to industry experience.  It can also
be used to test core software functionality such as tracing and repeatability.</p></li>
<li><p>Cropped - a subset of households and zones for efficient / portable running for testing.  This setup can really only
be used to test the software since model results are difficult to compare to observed/known answers.  This version of
an example is not recommended for testing overall runtime since it’s a convenience sample and may not represent the
true regional model travel demand patterns.  However, depending on the question, this setup may be able to answer
questions related to runtime, such as improvements to methods indifferent to the size of the population and number of
zones.</p></li>
<li><p>Other - a specific route/path through the code for testing.  For example, the estimation example tests the estimation
mode functionality.  The estimation example is a version of the example prototype MTC example - it inherits most settings from
prototype_mtc and includes additional settings for reading in survey files and producing estimation data bundles.</p></li>
</ul>
<p>Regardless of the type or version, all functioning examples are described in a common list stored in
<a class="reference external" href="https://github.com/ActivitySim/activitysim/blob/main/activitysim/examples/example_manifest.yaml">example_manifest.yaml</a>.
Each item included in this file represents one example, and each includes the following tags:</p>
<ul class="simple">
<li><p><em>name</em>: A unique name for the example, used to identify the example when using the <cite>activitysim create</cite> command. The
naming convention used is to give each example a name that is all lower case, and which uses underscores to separate
words.</p></li>
<li><p><em>description</em>: A short sentence describing the example.</p></li>
<li><p><em>include</em>: A list of files or directories to include in the example.  For smaller input files (e.g. configuration
files, or data files used on “test” sized examples), each file or directory to include can be given as a simple
string, which specifies the subdirectory of the embedded ActivitySim examples to be copied into a working directory.
For larger files that are not embedded into the main ActivitySim GitHub repository, items are given as a 3-tuple:
(url, target_path, sha256). The <cite>url</cite> points to a publicly available address where the file can be downloaded, the
<cite>target_path</cite> gives the relative filepath where the file should be installed in the working directory, and the
<cite>sha256</cite> is a checksum used to verify the file was downloaded correctly (and to prevent re-downloading when the file
is already available).  For defining new examples, use the <cite>sha256_checksum</cite> function to get a file’s checksum that
should be included in the example manifest.</p></li>
</ul>
</section>
<section id="id2">
<h3>Testing<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
<p>The test plan for test examples versus agency examples is different:</p>
<ul class="simple">
<li><p>Test examples test software features such as stability, tracing, expression solving, etc.  This set of tests is run
by the TravisCI system and is a central feature of the software development process.</p></li>
<li><p>Agency examples test a complete run of the cropped version to ensure it runs and the results are as expected.  This
is done via a simple run model test that runs the cropped version and compares the output trip list to the expected
trip list.  This is what is known as a regression test.  This test is also run by TravisCI.</p></li>
</ul>
<p>Both types of examples are stored in the ActivitySim repositories for version control and collaborative maintenance.
There are two storage locations:</p>
<ul class="simple">
<li><p>The <a class="reference external" href="https://github.com/ActivitySim/activitysim/tree/main/activitysim/examples">activitysim package example folder</a>,
which stores the test and agency example setup files, cropped data and cropping script, regression test script,
expected results, and a change log to track any revisions to the example to get it working for testing.  These
resources are the resources automatically tested by the TravisCI test system with each revision to the software.</p></li>
<li><p>The <a class="reference external" href="https://github.com/activitysim/activitysim_resources">activitysim_resources repository</a>, which stores just the
full scale example data inputs using <a class="reference external" href="https://git-lfs.github.com">Git LFS</a>.  This repository has a monthly cost and
takes time to upload/download and so the contents of it are separate from the main software repository.  These
resources are the resources periodically and manually tested (for now).</p></li>
</ul>
<p>This two-part solution allows for the main activitysim repo to remain relatively lightweight, while providing an
organized and accessible storage solution for the full scale example data.  The ActivitySim command line interface for
creating and running examples makes uses the
<a class="reference external" href="https://github.com/ActivitySim/activitysim/blob/main/activitysim/examples/example_manifest.yaml">example_manifest.yaml</a>
to maintain the dictionary of the examples and how to get and run them.</p>
</section>
<section id="running-the-test-system">
<h3>Running the Test System<a class="headerlink" href="#running-the-test-system" title="Permalink to this headline">#</a></h3>
<p>The automatic TravisCI test system runs the test examples and the cropped agency examples.  Examples of the testing
resources for each agency example that need to be up-to-date are:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/ActivitySim/activitysim/tree/main/activitysim/examples/prototype_semcog/scripts">scripts folder (including crop script)</a></p></li>
<li><p><a class="reference external" href="https://github.com/ActivitySim/activitysim/main/main/activitysim/examples/prototype_semcog/test">test folder (including test script)</a></p></li>
<li><p><a class="reference external" href="https://github.com/ActivitySim/activitysim/tree/main/activitysim/examples/prototype_semcog/test/regress">regress folder (including expected outputs)</a></p></li>
</ul>
<p>For the time being, running the full scale examples is done manually since it involves getting and running several large examples that take many hours to run.  The entire system could be fully automated, and either run in the cloud or on a local server.</p>
</section>
<section id="update-use-cases">
<h3>Update Use Cases<a class="headerlink" href="#update-use-cases" title="Permalink to this headline">#</a></h3>
<p>To better illustrate the workflow for adding agency examples, a series of use cases is discussed.</p>
<p>When a new version of the code is pushed to develop:</p>
<ul class="simple">
<li><p>The automatic test system is run to ensure the tests associated with the test examples pass.  If any of the tests do not pass, then either the code or the expected test results are updated until the tests pass.</p></li>
<li><p>The automatic test system also runs each cropped agency example regression test to ensure the model runs and produces the same results as before.  If any of the tests do not pass, then either the code or the expected test results are updated until the tests pass.  However, the process for resolving issues with agency example test failure has two parts:</p>
<ul>
<li><p>If the agency example previous ran without error or future warnings (i.e. deprecation warnings and is therefore up-to-date), then the developer will be responsible for updating the agency example so it passes the tests</p></li>
<li><p>If the agency example previously threw errors or future warnings (i.e. is not up-to-date), then the developer will not update the example and the responsibility will fall to the agency to update it when they have time.  This will not preclude development from advancing since the agency specific test can fail while the other tests continue to pass.  If the agency example is not updated within an agreed upon time frame, then the example is removed from the test system.</p></li>
</ul>
</li>
</ul>
<p>To help understand this case, the addition of support for representative logsums to <a class="reference internal" href="examples.html#prototype-mtc"><span class="std std-ref">prototype_mtc</span></a> is discussed.  prototype_mtc was selected as the test case for development of this feature because this feature could be implemented and tested against this example, which is the primary example to date.  With the new feature configured for this example, the automatic test system was run to ensure all the existing test examples pass their tests.  The automatic test system was also run to ensure all the cropped agency examples passed their tests, but since not of them include this new feature in their configuration, the test results were the same and therefore the tests passed.</p>
<p>When an agency wants to update their example:</p>
<ul class="simple">
<li><p>It is recommended that agencies keep their examples up-to-date to minimize the cost/effort of updating to new versions of ActivitySim.  However, the frequency with which to make that update is a key issue.  The recommended frequency of ensuring the agency example is up-to-date depends on the ActivitySim development roadmap/phasing and the current features being developed.  Based on past project experience, it probably makes sense to not let agency examples fall more than a few months behind schedule, or else updates can get onerous.</p></li>
<li><p>When making an agency model update, agencies update their example through a pull request.  This pull request changes nothing outside their example folder.  The updated resources may include updated configs, inputs, revisions to the cropped data/cropping script, and expected test results.  The automatic cropped example test must run without warnings.  The results of the full scale version is shared with the development team in the PR comments.</p></li>
</ul>
<p>To help understand this case, the inclusion of <a class="reference internal" href="examples.html#placeholder-psrc"><span class="std std-ref">placeholder_psrc</span></a> as an agency example is discussed.  This model is PSRC’s experimentation of a two zone model and is useful for testing the two zone features, including runtime.  A snapshot of PSRC’s efforts to setup an ActivitySim model with PSRC inputs was added to the test system as a new agency example, called placeholder_psrc.  After some back and forth between the development team and PSRC, a full scale version of placeholder_psrc was successfully run.  The revisions required to create a cropped version and full scale version were saved in a change log included with the example.  When PSRC wants to update placeholder_psrc, PSRC will pull the latest develop code branch and then update placeholder_psrc so the cropped and full scale example both run without errors.  PSRC also needs to update the expected test results.  Once everything is in good working order, then PSRC issues a pull request to develop to pull their updated example.  Once pulled, the automatic test system will run the cropped version of placeholder_psrc.</p>
<p>When an agency example includes new submodels and/or contributions to the core that need to be reviewed and then pulled/accepted:</p>
<ul class="simple">
<li><p>First, the agency example must comply with the steps outlined above under “When an agency wants to update their example”.</p></li>
<li><p>Second, the agency example must be up-to-date with the latest develop version of the code so the revisions to the code are only the exact revisions for the new submodels and/or contributions to the core.</p></li>
<li><p>The new submodels and/or contributions to the core will then be reviewed by the repository manager and it’s likely some revisions will be required for acceptance.  Key items in the review include python code, user documentation, and testable examples for all new components.  If the contribution is just new submodels, then the agency example that exercises the new submodel is sufficient for test coverage since TravisCI will automatically test the cropped version of the new submodel.  If the contribution includes revisions to the core that impact other test examples, then the developer is responsible for ensuring all the other tests that are up-to-date are updated/passing as well.  This includes other agency examples that are up-to-date.  This is required to ensure the contribution to the core is adequately complete.</p></li>
</ul>
<p>To help understand this case, the addition of the parking location choice model for <a class="reference internal" href="examples.html#prototype-arc"><span class="std std-ref">prototype_arc</span></a> is discussed.  First, ARC gets their example in good working order - i.e. updates to develop, makes any required revisions to their model to get it working, creates a cropped and full scaled example, and creates the expected test results.  In addition, this use case includes additional submodel and/or core code so ARC also authors the new feature, including documentation and any other relevant requirements such as logging, tracing, support for estimation, etc.  With the new example and feature working offline, then ARC issues a pull request to add prototype_arc and the new submodel/core code and makes sure the automatic tests are passing.  Once accepted, the automatic test system will run the test example tests and the cropped agency examples.  Since the new feature - parking location choice model - is included in prototype_arc, then new feature is now tested.  Any testing of downstream impacts from the parking location choice model would also need to be implemented in the example.</p>
</section>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="dev-guide/install.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Developer Installation</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="models.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Models</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <!-- This will display the version of the docs -->
<p class="last-updated">
ActivitySim 1.1.1.dev41+g4944aef4, documentation last updated Aug 25, 2022.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 4.5.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>